{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 3 : Normal Distribution\n",
    "#                                       Introduction to Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability theory is primarily concerned with calculating various quantities associated with a probability model. This requires that we know what the correct probability model is. In applications, this is often not the case, and the best we can say is that the correct probability measure to use is in a set of possible probability measures. We refer to this collection as the statistical model. So, in a sense, our uncertainty has increased; not only do we have the uncertainty associated with an outcome or response as described by a probability measure, but now we are also uncertain about what the probability measure is.\n",
    "Statistical inference is concerned with making statements or inferences about characteristics of the true underlying probability measure. Of course, these inferences must be based on some kind of information; the statistical model makes up part of it. Another important part of the information will be given by an observed outcome or response, which we refer to as the data. Inferences then take the form of various statements about the true underlying probability measure from which the data were obtained. These take a variety of forms, which we refer to as types of inferences.\n",
    "\n",
    "### Summary \n",
    "- Statistics is applied to situations in which we have questions that cannot be answered definitively, typically because of variation in data.\n",
    "- Probability is used to model the variation observed in the data. Statistical inference is concerned with using the observed data to help identify the true probability distribution (or distributions) producing this variation and thus gain insight into the answers to the questions of interest.\n",
    "\n",
    "In general when you have a population to study, we are often presented with data that are supposed to be produced by a random mechanism. In other words, suppose we are in a situation involving a measurement $X$, whose distribution is unknown, and we have obtained the data (realizations of the random variable $X$) $(x_1, x2,\\ldots , x_n)$ , i.e., observed $n$ values of $X$.\n",
    "\n",
    "What we do now with the data depends on two things : First, we have to determine what we want to know about the underlying population distribution. Second, we have to use statistical theory to combine the data with the statistical model to make inferences about the characteristics of interest. \n",
    "\n",
    "In practice, for a big population, it is not possible (or it is very time consuming) to collect information about all population individuals. Therefore, the true statistical model of the whole population is very often unknown. Hence, a part of the population called a sample of the population is studied and the statistical inference on the whole population is performed based on the statistical analysis of the sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Descriptive Statistics\n",
    "\n",
    "Descriptive statistics are often used as a preliminary step before more formal inferences are drawn and are justified on simple intuitive grounds. They are called descriptive because they are estimating quantities that describe features of the underlying distribution. In descriptive statistics step, in general, statisticians often focus on various characteristics of distributions such as Estimating Proportions and Cumulative Proportions, Estimating Quantiles and Measuring Location and Scale of a Population Distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you have a dataset data3 on which we have no information. We can use the read command to load the files (RData, CSV etc). Thereafter, we perform descriptive statistics for the data to get some information.\n",
    "\n",
    "### Loading and understanding the dataset :\n",
    "You can load the data by load(\"data3.RData\") or read.csv2(\"data3.csv\"). Click on the Run button each time at the top of the page to place a command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "5000"
      ],
      "text/latex": [
       "5000"
      ],
      "text/markdown": [
       "5000"
      ],
      "text/plain": [
       "[1] 5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.0560749636262055</li>\n",
       "\t<li>-0.540232370531951</li>\n",
       "\t<li>-0.936734755249935</li>\n",
       "\t<li>-0.0625219294348006</li>\n",
       "\t<li>-1.99577494675951</li>\n",
       "\t<li>-0.920731127165091</li>\n",
       "\t<li>-1.0931547909583</li>\n",
       "\t<li>-1.58216564705119</li>\n",
       "\t<li>-0.637982578864526</li>\n",
       "\t<li>-0.752158953741083</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.0560749636262055\n",
       "\\item -0.540232370531951\n",
       "\\item -0.936734755249935\n",
       "\\item -0.0625219294348006\n",
       "\\item -1.99577494675951\n",
       "\\item -0.920731127165091\n",
       "\\item -1.0931547909583\n",
       "\\item -1.58216564705119\n",
       "\\item -0.637982578864526\n",
       "\\item -0.752158953741083\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.0560749636262055\n",
       "2. -0.540232370531951\n",
       "3. -0.936734755249935\n",
       "4. -0.0625219294348006\n",
       "5. -1.99577494675951\n",
       "6. -0.920731127165091\n",
       "7. -1.0931547909583\n",
       "8. -1.58216564705119\n",
       "9. -0.637982578864526\n",
       "10. -0.752158953741083\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  0.05607496 -0.54023237 -0.93673476 -0.06252193 -1.99577495 -0.92073113\n",
       " [7] -1.09315479 -1.58216565 -0.63798258 -0.75215895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# R code: In R, run the followings:\n",
    "load(\"data3.RData\") # or read.csv2(\"data3.csv\")\n",
    "mode(data3)\n",
    "is.vector(data3)\n",
    "length(data3)\n",
    "data3[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the information above means is that, the data3 is a collection of numerical objects and it is a vector of length 5000. For a vector of real values, we can consider a random variable $X$ whose realizations are summarized in the vector data3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 'summary' :\n",
    "summary() is one of the most important functions that help in summarising each attribute in the dataset. It gives a set of descriptive statistics, depending on the type of variable. In case of a Numerical Variable, it gives Mean, Median, Mode, Range and Quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Clik here and Use summary function for data3 and then Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here and write the interpretation of the results obtained for summary() and then Run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation and plotting the data :\n",
    "Data Visualisation is an art of turning data into insights that can be easily interpreted. For discrete quantitative variables, we can plot\n",
    " the sample proportions (relative frequencies). For continuous quantitative variables, we can introduce the density histogram. These plots give us some idea of the shape of the distribution from which we are sampling. For example, we can see if there is any evidence that the distribution is strongly skewed. Examples : Histograms, Boxplots and Outliers or Bar Charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, using hist() function, make a histogram for the data3, then Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, write the interpretation of the histogram obtained for data3 :\n",
    "- what does a histogram means ?\n",
    "- Based on the histrogram plot, which continuous distribution function can be considered for data3 ? Why ?\n",
    "and then Run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Types of Inference\n",
    "Certainly quoting descriptive statistics and plotting the data are methods used by a statistician to try to learn something about the underlying population distribution. There are difficulties with this approach, however, as we have just chosen these methods based on intuition. Often it is not clear which descriptive statistics we should use. Furthermore, these data summaries make no use of the information we have about the true population distribution as expressed by the statistical model, namely, $f_X$ which depends on a (or more than one) parameter $\\theta$. Taking account of this information leads us to develop a theory of statistical inference, i.e., to specify how we should combine the model information together with the data to make inferences about population quantities. We will do this by Likelihood (or classical) Inference and Bayesian Inference. In a statistical application, we do not know $f$ as a function of the unknown parameter $\\theta$ (Parametric distributions); we know only that $f\\in \\{f_{\\theta} : \\theta \\in \\Theta\\}$ ($\\Theta$ is the set of all possible values of $\\theta$), and we observe the data $x$. We are uncertain about which candidate $f_{\\theta}$ is correct, or, equivalently, which of the possible values of $\\theta$ is correct. Hence, if $\\phi(\\theta)$ is a fonction of the unknown parameter $\\theta$, three types of inference are considered for $\\phi(\\theta)$ :\n",
    "\n",
    "- Choose an estimate $T(x)$ of $\\phi(\\theta)$, referred to as the problem of estimation,\n",
    "- Construct a subset $C(x)$ of the set of possible values for $\\phi(\\theta)$ that we believe contains the true value, referred to as the problem of credible region or confidence region construction,\n",
    "- Assess whether or not $\\phi_0$ is a plausible value for $\\phi(\\theta)$ after having observed $x$, referred to as the problem of hypothesis assessment.\n",
    "\n",
    "### Likelihood Inference\n",
    "Likelihood inferences are based only on the data $x$ and the model $\\{P_{\\theta} : \\theta \\in\\theta\\}$, the set of possible probability measures for the system under investigation. From these ingredients we obtain the basic entity of likelihood inference, namely, the likelihood function.\n",
    "#### Likelihood function\n",
    "Having $n$ observed data $x_1,\\ldots,x_n$, consider the function $\\ell(\\theta | x_1,\\ldots,x_n)$ defined on the parameter space $\\Theta$ and taking values in $\\mathbb{R}$, given by $\\ell(\\theta | x) = f_{X}(x,\\theta)$($\\theta$ is supposed to be unknown and $f$ is therefore a function of $x$ and $\\theta$). We refer to $\\ell(\\theta | x)$ as the likelihood function determined by the model and the data. The value $\\ell(\\theta | x)$ is called the likelihood of $\\theta$.\n",
    "\n",
    "#### Inference on $\\theta$ using Maximum Likelihood Estimation (MLE)\n",
    "When we are interested in a point estimate of $\\theta$, then a value $\\hat{\\theta}$ that maximizes $\\ell(\\theta | x_1,\\ldots,x_n)$ is a sensible choice, as this value is the best supported by the data, i.e., \n",
    "$$\n",
    "\\forall \\theta \\in \\Theta : \\ell(\\hat{\\theta} | x_1,\\ldots,x_n)\\geq \\ell(\\theta | x_1,\\ldots,x_n).\n",
    "$$\n",
    "##### Computation of MLE\n",
    "An important issue is the computation of MLEs and rather than using the likelihood function, it is often convenient to use the log-likelihood function, e.i., calculate the maximum of $\\ln(\\ell(\\theta | x_1,\\ldots,x_n))$.\n",
    "\n",
    "For data3, we suppose that $X\\sim \\mathcal{N}(m=-1,\\sigma^2)$ and only $\\sigma^2$ is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- calculate the density function of $f_X(x)$ of $X$,\n",
    "- calculate the likelihood function of $X=x_1, \\ldots, x_n$ i.i.d. as Normal distribution with parameter $\\sigma^2$,\n",
    "$$\\ell(\\sigma^2 | x_1, \\ldots, x_n)=\\prod_{i=1}^nf_X(x_i)$$\n",
    "- calculate the logarithm function of $\\ell$, \n",
    "$$\\ln(\\ell(\\sigma^2 | x_1, \\ldots, x_n))$$\n",
    "- calculate $\\hat{\\sigma^2}$ as the maximum of $\\ln(\\ell(\\sigma^2 | x_1, \\ldots, x_n))$\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, calculate MLE=sigma^2_hat for sigma^2 using data3, then Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences Based on the MLE\n",
    "From a classical point of view, the MLE $\\hat{\\theta}$ is a natural estimate of the true value of $\\theta$. However, in practice, we want to know how reliable the estimate $\\hat{\\theta}$ is. \n",
    "\n",
    "#### Mean-Squared error\n",
    "Perhaps the most commonly used measure of the accuracy of a general estimator $T(x)$ of $\\phi(\\theta)$ is what we call the mean-squared error :\n",
    "$$\n",
    "\\forall \\theta : MSE_{\\theta}(T(x))=\\mathbb{E}(T(x)-\\phi(\\theta))^2.\n",
    "$$\n",
    "which measures the difference between the estimate and the true value of the parameter. Clearly, the smaller $MSE_{\\theta}(T(x))$ is, the more concentrated the sampling distribution of $T(x)$ is about the value $\\phi(\\theta)$.\n",
    "\n",
    "Now suppose again that $X=x_1, \\ldots, x_n$ i.i.d. as Normal distribution with parameter $\\sigma^2$, $\\phi(\\theta)=\\sigma^2$ and $T(x)=\\hat{\\sigma^2}$ where $\\hat{\\sigma^2}$ is the MLE you have written above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- calculate the $MSE_{\\sigma^2}(\\hat{\\sigma^2})$ :\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset data3, we suppose that the true value of $\\sigma^2$ is $1$, and the estimate of $\\sigma^2$ is supposed to be the MLE $\\hat{\\sigma^2}$ that you have calculated in R code part above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, calculate the MSE_{\\sigma^2}(\\hat{\\sigma^2}), then Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write the interpretation of :\n",
    "- $MSE_{\\sigma^2}$,\n",
    "- Is MLE a good estimation of the true $\\sigma^2$ ? why ?\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "Another measure of the accuracy is the bais :  The bias in the estimator $T$ of \n",
    "$\\phi(\\theta)$ is given by $\\mathbb{B}(T)=\\mathbb{E}(T) - \\phi(\\theta)$ whenever $\\mathbb{E}(T)$ exists.\n",
    "When the bias in an estimator $T$ is $0$ for every $\\theta$, we call $T$ an unbiased estimator\n",
    "of $\\phi(\\theta)$, i.e., $T$ is unbiased whenever $\\mathbb{E}(T) = \\phi(\\theta)$ for every \n",
    "$\\theta\\in \\Theta$.\n",
    "\n",
    "Now, for $X=x_1, \\ldots, x_n$ i.i.d. as Normal distribution with $\\sigma^2$ if $T=\\hat{\\sigma^2}$, the MLE and $\\phi(\\theta)=\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- calculate $\\mathbb{B}(\\hat{\\sigma^2})$,\n",
    "- Is $\\hat{\\sigma^2}$ unbiased for $\\sigma^2$ ? why?\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, calculate the B(sigma^2_hat) above, then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency of estimators\n",
    "Perhaps the most important property that any estimator $T$ of a characteristic $\\phi(\\theta)$ can have is that it be consistent. Broadly speaking, this means that as we increase the amount of data we collect, then the sequence of estimates should converge to the true value of $\\phi(\\theta)$. We can show that the MLE is a consistent estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Confidence Intervals\n",
    "While the standard error seems like a reasonable quantity for measuring the accuracy of an estimate of $\\phi(\\theta)$, its interpretation is not entirely clear at this point. It turns out that this is intrinsically tied up with the idea of a confidence interval. An interval $C(x) = (l(x),u(x))$ is a $\\gamma$-confidence interval for $\\phi(\\theta)$ if $P(\\phi(\\theta)\\in C(x)) = P(l(x)\\leq \\phi(\\theta) \\leq u(x)) \\geq \\gamma$ for every $\\theta\\in \\Theta$. We refer to $\\gamma$ as the confidence level of the interval.\n",
    "\n",
    "##### Computation of confidence intervals\n",
    "The classical method of computing the confidence interval for a parameter is that based on the likelihood function, a natural way is to consider the following restriction for such a region :\n",
    "$$\n",
    "C(x_1, \\ldots, x_n)=\\{\\theta : \\ell(\\theta|x_1, \\ldots, x_n) \\geq k(x_1, \\ldots, x_n)\\}\n",
    "$$\n",
    "\n",
    "For $X=x_1, \\ldots, x_n$ i.i.d. as Normal distribution $\\sigma^2$, it is difficult to obtain an interval for $\\sigma^2$ based on $\n",
    "\\ell(\\sigma^2 | x_1, \\ldots, x_n)\\geq k(x_1, \\ldots, x_n)\n",
    "$\n",
    "In such a case, we use the asymptotic confidence interval which is based on the (asymptotic) distribution of the MLE based on Central Limit Theorem (all dsitributions can be approximated by normal distribution). In other words, for $X_1, \\ldots, X_n$ i.i.d., from the central limit theorem, we know that\n",
    "$$\n",
    "Z=\\sqrt{n}\\frac{(T(x_1,\\ldots,x_n)-\\sigma^2)}{2\\sigma^2}\\sim \\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "where $T(x_1,\\ldots,x_n)=\\frac{1}{n}\\sum_{i=1}^n(x_i-m)^2$. Finally, for a $\\gamma$ known, based on a standard normal distribution, we can calculate the values of $-z, z$ for $z\\geq 0$ in the following equation using the cdfs table\n",
    "$$\n",
    "P(-z\\leq Z\\leq z)=\\gamma.\n",
    "$$\n",
    "and so we obtain a confidence interval based on the value of $z$.\n",
    "To understand how we calculate a confidence interval for $\\sigma^2$ using the asymptotic distribution above do :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- from the Standard normal cdfs table, calculate $z\\geq 0$ in  \n",
    "$\n",
    "P(-z\\leq Z\\leq z)=\\gamma\n",
    "$ if $\\gamma=0.95$\n",
    "- now, for obtained value of $z$ write the following equation as an interval for $\\sigma^2$\n",
    "$$\n",
    "-z\\leq \\sqrt{n}\\frac{(T(x_1,\\ldots,x_n)-\\sigma^2)}{2\\sigma^2}\\leq z\n",
    "$$\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, calculate T(x_1,...,x_n) for data3\n",
    "# then for obtained value of T(x_1,...,x_n), calculate the confidence interval above, then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Hypotheses\n",
    "As mentioned above, another class of inference procedures is concerned with what we call hypothesis assessment. Suppose there is a hypothesis, that specifies a value for a characteristic of interest $\\phi(\\theta)$, say $\\phi(\\theta) = \\phi_0$. Often this hypothesis is written \n",
    "$$\n",
    "H_0 : \\phi(\\theta) = \\phi_0\n",
    "$$\n",
    " and is referred to as the null hypothesis. The word null is used because the value specified in $H_0$ is often associated with a treatment having no effect. For example, if we want to assess whether or not a proposed new drug does a better job of treating a particular condition than a standard treatment does, the null hypothesis will often be equivalent to the new drug providing no improvement.\n",
    "\n",
    "The statistician is then charged with assessing whether or not the observed $x$ is in accord with this hypothesis. So we wish to assess the evidence in $x$ for $\\phi(\\theta) = \\phi_0$ being true. A statistical procedure that does this can be referred to as a hypothesis assessment, a test of significance, or a test of hypothesis. Such a procedure involves measuring how surprising the observed $x$ is when we assume $H_0$ to be true. It is clear that $x$ is surprising whenever $x$ lies in a region of low probability for each of the distributions specified by the null hypothesis. If we decide that the data are surprising under $H_0$, then this is evidence against $H_0$. This assessment is carried out by calculating a probability, called a P-value, so that small values of the P-value indicate that $x$ is surprising.\n",
    "\n",
    "##### How to perform a hypothesis test\n",
    "In practice, we always consider another hypothesis against $H_0$, called alternative hypothesis $H_1$, and we say that the null hypothesis can not be accepted if the evidence in $x$ for  the alternative hypothesis being true is stronger than for $H_0$. In other words, for our Normal distribution if $$\n",
    "H_0 : \\sigma^2 = \\sigma^2_0 \\quad \\text{against} \\quad H_1 : \\sigma^2=\\sigma^2_1,\n",
    "$$\n",
    "where $\\sigma^2_0, \\sigma^2_1$ are supposed to be known, then $H_0$ is not accepted when\n",
    "$$\n",
    "\\ell(\\sigma^2_0 | x_1, \\ldots, x_n)<\\ell(\\sigma^2_1 | x_1, \\ldots, x_n)\n",
    "$$\n",
    "or equivalently\n",
    "$$\n",
    "\\frac{\\ell(\\sigma^2_0 | x_1, \\ldots, x_n)}{\\ell(\\sigma^2_1 | x_1, \\ldots, x_n)}<c\n",
    "$$\n",
    "where $c$ is a constant. The inequality above gives us what we call the rejection region of $H_0$, $R_{H_0}$ as a function of $T(X_1,\\ldots,X_n)$ and $T$ is called the test statistics. In practice, $R_{H_0}$ is an inequality of the form $T(X_1,\\ldots,X_n)>k$ or $T(X_1,\\ldots,X_n)<k$ where $k$ is a function of $c$. Then, we calculate P-value, the probability of $R_{H_0}$ under the null hypothesis (assuming the null hypothesis is true) based on the observations $T(x_1,\\ldots,x_n)$.\n",
    "Thereafter, we select a significance level $\\alpha$ that is a probability threshold below which the null hypothesis will be rejected and finally reject the null hypothesis if P-value$\\leq \\alpha$. \n",
    "\n",
    "Now, to understand how we make a decision about the parameter $\\sigma^2$, using testing hypotheses, suppose that $\\sigma^2_0=0.9$ and $\\sigma^2_1=4$. Then do :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- calculate $R_{H_0}$ as a function of $T(X_1,\\ldots,X_n)=\\frac{1}{n}\\sum_{i=1}^n(X_i+1)^2$ from \n",
    "$$\n",
    "\\frac{\\ell(\\sigma^2_0 | X_1, \\ldots, X_n)}{\\ell(\\sigma^2_1 | X_1, \\ldots, X_n)}<c\n",
    "$$\n",
    "- $R_{H_0}$ that you obtain may have the form $T(X_1,\\ldots,X_n)>k$ or $T(X_1,\\ldots,X_n)<k$. Replace $k$ in your $R_{H_0}$ by $T(x_1,\\ldots,x_n)$ where $x_1,\\ldots,x_n$ are data3,\n",
    "- under $H_0$ and by using the central limit theorem for $T(x_1,\\ldots,x_n)$, calculate the P-value\n",
    "$$\n",
    "P(R_{H_0}|H_0)\n",
    "$$\n",
    "- compare the obtained P-value with the significance level $\\alpha=0.01$,\n",
    "- can we accept $H_0$ ? Why ?\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference\n",
    "At the heart of the theory of inference is the concept of the statistical model $\\{P_{\\theta} : \\theta\\in \\Theta\\}$ that describes the statisticianâ€™s uncertainty about how the observed data were produced. Previous section dealt with the analysis of this uncertainty based on the model and the data alone. In some cases, this seemed quite successful, but we note that we only dealt with some of the simpler contexts there.\n",
    "If we accept the principle that, to be amenable to analysis, all uncertainties need to be described by probabilities, then the prescription of a model alone is incomplete, as this does not tell us how to make probability statements about the unknown true value of $\\theta$. In this section, we complete the description so that all uncertainties are described by probabilities. This leads to a probability distribution for $\\theta$, and the parameter now plays the role of the unobserved response or a new random variable. This is the Bayesian approach to inference.\n",
    "\n",
    "#### Prior and posterior distributions\n",
    "The Bayesian model for inference contains the statistical model $\\{P_{\\theta} : \\theta\\in \\Theta\\}$ for the data $x$ and adds to this the prior probability measure $\\pi(\\theta)$ for $\\theta$. The prior describes the statistician's beliefs about the true value of the parameter $\\theta$ a priori, i.e., before observing the data. After observing the data, the relevant distribution to use in making probability statements about $\\theta$ is the conditional distribution of $\\theta$ given $x$. We denote this conditional probability measure by $\\pi(\\theta | x)$ and refer to it as the posterior distribution of $\\theta$. Using Bayes theorem, the posterior density, or posterior probability function is given by\n",
    "$$\n",
    "\\pi(\\theta | x)=\\frac{\\pi(\\theta)P_{\\theta}(x)}{\\int_{\\mathbb{R}}\\pi(\\theta)P_{\\theta}(x)d\\theta}.\n",
    "$$\n",
    "which is written as following\n",
    "$$\n",
    "\\pi(\\theta | x)\\propto \\pi(\\theta)P_{\\theta}(x).\n",
    "$$\n",
    "\n",
    "To understand how we calculate the posterior distribution for our $\\sigma^2$, first suppose an Inverse-Gamma distribution as $\\sigma^2\\sim \\mathcal{IG}(a, b)$ (see https://en.wikipedia.org/wiki/Inverse-gamma_distribution) as its prior distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- write $\\pi(\\sigma^2)=\\mathcal{IG}(a, b)$,\n",
    "- calculate\n",
    "$$\n",
    "\\pi(\\sigma^2 | x_1, \\ldots, x_n)\\propto \\pi(\\sigma^2)\\ell(\\sigma^2 | x_1, \\ldots, x_n),\n",
    "$$\n",
    "- from above, $\\pi(\\sigma^2 | x_1, \\ldots, x_n)$ looks like which density function ?\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter estimation\n",
    "Suppose now that we want to calculate an estimate of a characteristic of interest $\\phi(\\theta)$. We base this on the posterior distribution of this quantity. \n",
    "\n",
    "A first method which is the most natural estimate, is to obtain the posterior density (or probability function when relevant) of $\\phi(\\theta)$ and use the posterior mode , i.e., the point where the posterior probability or density function of $\\phi$ takes its maximum. It is denoted by MAP.\n",
    "\n",
    "\n",
    "For our parameter $\\sigma^2$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- calculate the logarithm of the posterior distribution of $\\sigma^2$\n",
    "$$\n",
    "ln(\\pi(\\sigma^2 | x_1, \\ldots, x_n))\n",
    "$$\n",
    "- calculate MAP : the maximum of $ln(\\pi(\\sigma^2 | x_1, \\ldots, x_n))$,\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, if a=1 and b=2, using data3, calculate the value of MAP, then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative estimate is commonly used and has a natural interpretation. This is given by the posterior mean\n",
    "$$\n",
    "\\mathbb{E}(\\phi(\\theta)|x),\n",
    "$$\n",
    "whenever this exists. \n",
    "\n",
    "For $\\sigma^2$, do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- using $\\pi(\\sigma^2 | x_1, \\ldots, x_n)$, calculate \n",
    "$$\n",
    "\\mathbb{E}(\\sigma^2|x_1, \\ldots, x_n)=\\int_0^{\\infty} \\sigma^2 \\pi(\\sigma^2 | x_1, \\ldots, x_n) d\\sigma^2\n",
    "$$\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R code: Click here, if a=1 and b=2, using data3, \n",
    "# calculate the value of E(sigma^2|x_1, ..., x_n), then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click here, then write :\n",
    "- if the true value of $\\sigma^2$ is $1$, then compare it with the obtained MAP and $\\mathbb{E}(\\sigma^2|x_1, \\ldots, x_n)$,\n",
    "- interperate the comparison between MLE, MAP, $\\mathbb{E}(\\sigma^2|x_1, \\ldots, x_n)$ and the true $\\sigma^2$\n",
    ", then Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the posterior distribution of $\\phi$ is symmetrical about its mode, and the expectation exists, then the posterior expectation is the same as the posterior mode; otherwise, these estimates will be different.\n",
    "\n",
    "It can be shown that the Bayesian inference methods are consistent, e.i., as the sample size increases, the posterior distribution of $\\theta$ converges to a distribution degenerate at the true value. We can hence calculate the credible intervals and perform hypothesis testing using Bayesian methods. However,  in most cases, the posterior distribution has not a standard statistical model form and therefore, we need to use some numerical methods called Markov Chain Monte Carlo methods in order to approximate the parameter estimation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
